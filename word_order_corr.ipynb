{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2dcf72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "from lemminflect import getInflection\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf38748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_correction(f):\n",
    "    def get_correction(*args):\n",
    "        \n",
    "        correction = f(*args)\n",
    "        \n",
    "        correction = re.sub(' +', ' ', correction)\n",
    "        correction = re.sub(r'\\s(?=[?.;:,!])', r'', correction)\n",
    "        correction = re.sub(r'(\\s(?=n[^o]t\\b)|(?<=[Cc]an)\\s(?=not))', r'', correction)\n",
    "        correction = correction.strip()\n",
    "        if correction: \n",
    "            correction = correction[0].upper() + correction[1:] \n",
    "        return correction\n",
    "    \n",
    "    return get_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32630e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subj(pred):\n",
    "    \n",
    "    # simple cases\n",
    "    subjects = [child for child in list(pred.children) if child.dep_.startswith(('nsubj', 'csubj'))]\n",
    "    \n",
    "    # if predicate is an auxiliary, we want to take subjects of its head\n",
    "    if pred.dep_.startswith('aux'):\n",
    "        subjects += [child for child in list(pred.head.children) if child.dep_.startswith(('nsubj', 'csubj'))]\n",
    "        \n",
    "    # handling 'there is' and 'there are' cases\n",
    "    if 'there' in list(i.lower_ for i in pred.children):\n",
    "        subjects += [child for child in list(pred.children) if child.dep_ == 'attr']\n",
    "        \n",
    "    # handling misconstructed indirect questions e.g.\n",
    "    # \"He must understand what is happiness for him\"\n",
    "    # to consider \"happiness\" the subject not \"who\"\n",
    "    if subjects and subjects[0].tag_[0] == 'W' \\\n",
    "                and pred.doc[-1].text != '?' \\\n",
    "                and pred.dep_ != 'relcl' \\\n",
    "                and 'attr' in [t.dep_ for t in pred.children] \\\n",
    "                and pred.nbor().tag_ != 'JJR':\n",
    "        try: \n",
    "            s = next(t for t in pred.children if t.dep_ == 'attr')\n",
    "            subjects[0] = s\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    # handling conjuncts: multiple subjects as in 'Mother and father are key figures in a child's life'.\n",
    "    add_subj = []\n",
    "    for subject in subjects:\n",
    "        add_subj += list(subject.conjuncts)\n",
    "\n",
    "    cur_pred = pred\n",
    "    while len(subjects) == 0 and cur_pred.dep_ == \"conj\":\n",
    "        cur_pred = cur_pred.head\n",
    "        subjects = find_subj(cur_pred)\n",
    "\n",
    "    subjects += add_subj\n",
    "\n",
    "    # the subjects' order may be different from sentence order, so we arrange it right\n",
    "    subjects.sort(key=lambda subj: subj.i)\n",
    "    \n",
    "    # the subjects' order may be different from order in sentence, so we rearrange it\n",
    "    subjects.sort(key=lambda subj: subj.i)\n",
    "        \n",
    "    return subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd48e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pred_subj(doc):\n",
    "    \n",
    "    pred_sub = list()\n",
    "    predicates = []\n",
    "    conj_pred = dict()\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['AUX', 'VERB']:\n",
    "                 \n",
    "            # for analytical predicates\n",
    "            if token.tag_ in ['VBN', 'VBG', 'VB']:\n",
    "                aux = None\n",
    "                aux_2 = None\n",
    "                children = list(token.children)\n",
    "                for ch in children:\n",
    "                    if ch.dep_[:3] == 'aux' and ch.pos_ in ['VERB', 'AUX']: \n",
    "                        if ch.tag_ != 'VBN' and not aux:\n",
    "                            aux = ch\n",
    "                        else:\n",
    "                            aux_2 = ch\n",
    "                if aux:\n",
    "                    if aux_2:\n",
    "                        pred_sub += [((aux, aux_2, token), find_subj(aux))]\n",
    "                    else:\n",
    "                        pred_sub += [((aux, token), find_subj(aux))]\n",
    "\n",
    "            # all other cases\n",
    "            elif token.dep_ in ['ROOT', 'ccomp', 'xcomp', 'acl', 'relcl', 'parataxis', 'advcl', 'pcomp']:\n",
    "                pred_sub += [(tuple([token]), find_subj(token))]\n",
    "\n",
    "            # conjuncts: when there are multiple predicates connected by conjunction\n",
    "            elif token.dep_ == 'conj' and token.head in predicates:\n",
    "                if find_subj(token) != find_subj(token.head):\n",
    "                    pred_sub += [(tuple([token]), find_subj(token))]\n",
    "            \n",
    "            predicates = [t[0][-1] for t in pred_sub]\n",
    "                        \n",
    "    for ps in pred_sub:\n",
    "        p, s = ps[0], ps[1]\n",
    "        conj = p[-1].conjuncts\n",
    "        for c in conj:\n",
    "            if find_subj(c) == s:\n",
    "                if conj_pred.get(token.head.i):\n",
    "                    conj_pred[token.head.i].append(c.i)\n",
    "                else:\n",
    "                    conj_pred[token.head.i] = [c.i]\n",
    "\n",
    "    return pred_sub, conj_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a98ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_chunk(sent, subject):\n",
    "    '''Extract the subject phrase, \n",
    "    i.e. the part of the sentence that contains\n",
    "    all the subjects' noun phrases'''\n",
    "    \n",
    "    subject_phrase = dict()\n",
    "    \n",
    "    subject_phrase['start'] = subject[0].i\n",
    "    subject_phrase['end'] = subject[0].i\n",
    "\n",
    "    if len(subject) > 1:\n",
    "        \n",
    "        ch = []        \n",
    "        for s in subject:\n",
    "            ch += list(s.children)\n",
    "        conjunctions = [i for i in ch if i.tag_ == 'CC']\n",
    "        if conjunctions:\n",
    "            subject_phrase['conj'] = conjunctions[-1].text\n",
    "        else:\n",
    "            subject_phrase['conj'] = None\n",
    "    s_chunks = []\n",
    "    for s in subject:\n",
    "        try:\n",
    "            s_chunks += [next(i for i in sent.noun_chunks if s in i)]\n",
    "        except StopIteration:                \n",
    "            s_chunks.append([s])\n",
    "    subject_phrase['start'] = s_chunks[0][0].i\n",
    "    subject_phrase['end'] = s_chunks[-1][-1].i\n",
    "    \n",
    "    subject_phrase['span'] = sent[subject_phrase['start']:subject_phrase['end']+1]\n",
    "    if (subject_phrase['span'][0].is_sent_start\n",
    "        and subject_phrase['span'][0].tag_ not in {'NNP', 'NNPS'}\n",
    "        and subject_phrase['span'][0].text != 'I'):\n",
    "        first_word = subject_phrase['span'][0].lower_\n",
    "        subject_phrase['phrase'] = ' '.join([first_word]+[subject_phrase['span'][1:].text])\n",
    "    else:\n",
    "        subject_phrase['phrase'] = subject_phrase['span'].text\n",
    "        \n",
    "    return subject_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ccfaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_chunk(sent, pred, conj_p):\n",
    "    '''Extract the object phrase,\n",
    "    i.e. the part of the sentence that ocntains all\n",
    "    conjuct predicates, if any'''\n",
    "    \n",
    "    pred_phrase = dict()\n",
    "    pred_phrase['start'] = min(p.i for p in pred)\n",
    "    pred_phrase['end'] = max(p.i for p in pred)\n",
    "    pred_phrase['predicates'] = [pred[-1]]\n",
    "    pred_phrase['n_conj'] = 1\n",
    "    \n",
    "    if conj_p.get(pred[-1].i):\n",
    "        last_c = max([c for c in conj_p[pred[-1].i]])\n",
    "        pred_phrase['end'] = max(last_c, pred[-1].i)\n",
    "        pred_phrase['n_conj'] = len(conj_p[pred[-1].i])+1\n",
    "        pred_phrase['predicates'] += list(sent[i] for i in conj_p[pred[-1].i])\n",
    "        \n",
    "    pred_phrase['span'] = sent[pred_phrase['start']:pred_phrase['end']+1]\n",
    "    try:\n",
    "        s = list(j.i for j in pred_phrase['span'] if j.pos_ != 'AUX' and not j.dep_.startswith(('nsubj', 'csubj')))[0]\n",
    "        e = list(j.i for j in list(pred_phrase['span'])[::-1] if j.pos_ != 'AUX' and not j.dep_.startswith(('nsubj', 'csubj')))[0]\n",
    "        pred_phrase['span_wo_aux'] = sent[s:e+1]\n",
    "    except IndexError:\n",
    "        pred_phrase['span_wo_aux'] = sent[pred_phrase['predicates'][0].i:pred_phrase['end']+1]\n",
    "    pred_phrase['phrase'] = pred_phrase['span'].text[0].lower()+pred_phrase['span'].text[1:]\n",
    "    pred_phrase['phrase_wo_aux'] = pred_phrase['span_wo_aux'].text[0].lower()+pred_phrase['span_wo_aux'].text[1:]\n",
    "    \n",
    "    # expanding contractions\n",
    "    if pred_phrase['phrase'].startswith(\"'m\"):\n",
    "        pred_phrase['phrase'] = 'am '+pred_phrase['phrase'][2:]\n",
    "    elif pred_phrase['phrase'].startswith(\"'re\"):\n",
    "        pred_phrase['phrase'] = 'are '+pred_phrase['phrase'][2:]\n",
    "    elif pred_phrase['phrase'].startswith(\"'s\"):\n",
    "        if pred_phrase['span'][0].lemma_ == 'be': pred_phrase['phrase'] = 'is '+pred_phrase['phrase'][2:]\n",
    "        elif pred_phrase['span'][0].lemma_ == 'have': pred_phrase['phrase'] = 'has '+pred_phrase['phrase'][3:]\n",
    "    elif pred_phrase['phrase'].startswith(\"'d\"):\n",
    "        if pred_phrase['span'][0].lemma_ == 'would': pred_phrase['phrase'] = 'would '+pred_phrase['phrase'][5:]\n",
    "        elif pred_phrase['span'][0].lemma_ == 'have': pred_phrase['phrase'] = 'had '+pred_phrase['phrase'][3:]\n",
    "    elif pred_phrase['phrase'].startswith(\"'ll\"):\n",
    "        if pred_phrase['span'][0].lemma_ == \"'ll\": pred_phrase['phrase'] = 'will '+pred_phrase['phrase'][4:]\n",
    "        \n",
    "\n",
    "    \n",
    "    _ = re.search(\"((?<=\\s)ca|(?<=^)ca)(?=(\\s|$))\", pred_phrase['phrase'])\n",
    "    if _ and 'ca' in [p.text for p in pred_phrase['predicates']]:\n",
    "        pred_phrase['phrase'] = pred_phrase['phrase'][:_.start()]+'can'+pred_phrase['phrase'][_.end():]\n",
    "    \n",
    "    return pred_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592d4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_between_after(sent, pred, subj):\n",
    "    '''Split sentence into three parts:\n",
    "    - before: everything before predicate or subject, whichever comes first\n",
    "    - in between: everything between predicate and subject\n",
    "    - after: everything after predicate or subject, whiever comes last'''\n",
    "    \n",
    "    if pred['start'] < subj['start']:\n",
    "        everything_before = sent[:pred['start']].text\n",
    "    else:\n",
    "        everything_before = sent[:subj['start']].text\n",
    "    if pred['end'] < subj['end']:\n",
    "        everything_in_between = sent[pred['end']+1:subj['start']].text\n",
    "        everything_after = sent[subj['end']+1:].text\n",
    "    else:\n",
    "        everything_in_between = sent[subj['end']+1:pred['start']].text\n",
    "        everything_after = sent[pred['end']+1:].text\n",
    "        \n",
    "    if everything_after.startswith((\"n't\", \"n`t\", \"n’t\")):\n",
    "        everything_after = 'not'+everything_after[3:]\n",
    "        \n",
    "    return everything_before, everything_in_between, everything_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ebfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "@clean_correction\n",
    "def general_q_simple_verb(sent, predicate, subject, conj_p):\n",
    "    '''Simple question, like *Is it necessary? Does John come by often?*'''\n",
    "    \n",
    "    correction = ''\n",
    "    \n",
    "    subj = subject_chunk(sent, subject)\n",
    "    pred = pred_chunk(sent, predicate, conj_p)\n",
    "    before, between, after = before_between_after(sent, pred, subj)        \n",
    "            \n",
    "    # strong verbs do not need an auxiliary for quesiton\n",
    "    if (predicate[0].lemma_ in {'be', 'must', 'can', 'could', 'dare'\n",
    "                               'should', 'shall', 'may', 'might'}) \\\n",
    "       or (predicate[0].lemma_ == 'have' and predicate[0].tag_ == 'AUX'):\n",
    "        #do we include 'need' here?\n",
    "        if pred['start'] > subj['start']:                    \n",
    "\n",
    "            correction = ' '.join([before, pred['phrase'], subj['phrase'], between, after])\n",
    "    \n",
    "    # for weak verbs we need to add an auxiliary\n",
    "    else:\n",
    "        if predicate[0].morph.to_dict()['Tense'] == 'Past':\n",
    "            aux = 'did'\n",
    "        elif (subject[0].lower_ in ['i', 'you'] or\n",
    "              subject[0].pos_ == 'ADJ' or\n",
    "              subject[0].morph.to_dict()['Number'] == 'Plur' or\n",
    "              len(subject) > 1 and subj['conj'] != 'or'):\n",
    "            aux = 'do'\n",
    "        else: \n",
    "            aux = 'does'\n",
    "            \n",
    "        lex_verb = ''\n",
    "        for word in pred['span']:\n",
    "            if word in pred['predicates']:\n",
    "                lex_verb += str(word._.inflect('VB')).lower()+' '\n",
    "            else:\n",
    "                lex_verb += word.text+' '\n",
    "        \n",
    "        correction = ' '.join([before, aux, subj['phrase'], between, lex_verb, after])\n",
    "        \n",
    "    \n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9efb70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@clean_correction\n",
    "def subject_q(sent, predicate, subject, conj_p):\n",
    "    '''Subject question, doesn't need auxiliaries and inversion'''\n",
    "    \n",
    "    correction = ''\n",
    "    \n",
    "    subj = subject_chunk(sent, subject)\n",
    "    pred = pred_chunk(sent, predicate, conj_p)\n",
    "    before, between, after = before_between_after(sent, pred, subj)\n",
    "        \n",
    "    if subj['start'] > pred['start']:\n",
    "        if len(predicate) == 1:\n",
    "        \n",
    "            correction  = ' '.join([before, subj['phrase'], between, pred['phrase'], after])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            aux2 = ''\n",
    "            aux1 = predicate[0]\n",
    "            if len(predicate) == 3:\n",
    "                aux2 = predicate[1].lower_\n",
    "                \n",
    "            if aux1.lemma_ == 'do':\n",
    "                for i in pred['predicates']:\n",
    "                    s = i._.inflect(aux1.tag_)\n",
    "                    pred['phrase_wo_aux'] = re.sub(r'(?<=\\b)'+i.text+r'(?=\\b)', s, pred['phrase_wo_aux'], count=1)\n",
    "                aux1 = ''\n",
    "            else:\n",
    "                aux1 = aux1.lower_\n",
    "\n",
    "            correction = ' '.join([before, subj['phrase'], aux1, between, aux2, pred['phrase_wo_aux'], after])        \n",
    "            \n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a147851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@clean_correction\n",
    "def analytical_verb_q(sent, predicate, subject, conj_p):\n",
    "    '''For questions that have an analytical predicate'''\n",
    "    \n",
    "    correction = ''\n",
    "    \n",
    "    subj = subject_chunk(sent, subject)\n",
    "    pred = pred_chunk(sent, predicate, conj_p)\n",
    "    before, between, after = before_between_after(sent, pred, subj)\n",
    "    \n",
    "    lex_pr = predicate[-1] # lexical verb\n",
    "    aux = predicate[:-1] # auxiliaries\n",
    "    aux_n = len(aux)\n",
    "        \n",
    "    a2 = lex_pr.i\n",
    "    a1 = aux[0].i\n",
    "    if aux_n == 2:\n",
    "        a2 = aux[1].i\n",
    "    l = lex_pr.i\n",
    "\n",
    "    \n",
    "    if not a1 < subj['start'] < a2 <= l:\n",
    "        aux2 = ''\n",
    "        aux1 = predicate[0]\n",
    "        if aux1.lower_ in [\"'d\", '`d', '’d']:\n",
    "            if aux1.lemma_ == 'would':\n",
    "                aux1 = 'would'\n",
    "            elif aux1.lemma_ == 'have':\n",
    "                aux1 = 'had'\n",
    "        elif aux1.lower_ in [\"'ll\", '`ll', '’ll']:\n",
    "            aux1 = 'will'\n",
    "        elif aux1.lower_ == 'ca':\n",
    "            aux1 = 'can'\n",
    "        else:\n",
    "            aux1 = aux1.lower_\n",
    "            \n",
    "        neg = ''\n",
    "        if sent[aux[0].i+1].lemma_ == 'not':\n",
    "            neg = 'not'\n",
    "            if pred['span_wo_aux'][0].lemma_ == 'not':\n",
    "                pred['phrase_wo_aux'] = pred['phrase_wo_aux'][3:]\n",
    "            \n",
    "        if len(predicate) == 3:\n",
    "            aux2 = predicate[1].lower_\n",
    "              \n",
    "        correction = ' '.join([before, aux1, subj['phrase'], between, neg, aux2, pred['phrase_wo_aux'], after])\n",
    "        \n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8c7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interrogative_WO(sent, pred_subj, conj_p):\n",
    "    '''Question'''\n",
    "    \n",
    "    correction = ''    \n",
    "    \n",
    "    predicate = pred_subj[0]\n",
    "    subject = pred_subj[1]\n",
    "    subj = subject_chunk(sent, subject)\n",
    "\n",
    "    if any(w.tag_[0] == 'W' for w in subj['span']):\n",
    "        # it's a subject question\n",
    "        correction = subject_q(sent, predicate, subject, conj_p)\n",
    "    elif len(predicate) == 1:\n",
    "        correction = general_q_simple_verb(sent, predicate, subject, conj_p)\n",
    "    elif len(predicate) > 1:\n",
    "        # we have an analytical predicate\n",
    "        correction = analytical_verb_q(sent, predicate, subject, conj_p)\n",
    "             \n",
    "\n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df381c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@clean_correction\n",
    "def possible_inversion(sent, pred_subj, conj_p, culprit):\n",
    "    '''A sentence where the predicate is negated, \n",
    "       could possibly be an inversion'''\n",
    "    \n",
    "    correction = ''\n",
    "    \n",
    "    predicate = pred_subj[0]\n",
    "    pred = pred_chunk(sent, predicate, conj_p)\n",
    "    subject = pred_subj[1]\n",
    "    subj = subject_chunk(sent, subject)\n",
    "    \n",
    "    before, between, after = before_between_after(sent, pred, subj)\n",
    "                \n",
    "    if culprit.i < subject[0].i < predicate[0].i:\n",
    "        \n",
    "            if len(predicate) == 1:\n",
    "                \n",
    "                if (predicate[0].lemma_ in {'be', 'must', 'can', 'could', 'dare'\n",
    "                                            'should', 'shall', 'may', 'might'}):\n",
    "                    correction = ' '.join([before, pred['phrase'], between, subj['phrase'], after])\n",
    "\n",
    "                else:\n",
    "                    tense = predicate[0].morph.to_dict().get('Tense')\n",
    "                    if tense == 'Past':\n",
    "                        aux = 'did'\n",
    "                    elif (subject[0].lower_ in ['i', 'you'] or\n",
    "                          subject[0].pos_ == 'ADJ' or\n",
    "                          subject[0].morph.to_dict()['Number'] == 'Plur' or\n",
    "                          len(subject) > 1 and subj['conj'] != 'or'):\n",
    "                        aux = 'do'\n",
    "                    else: \n",
    "                        aux = 'does'\n",
    "\n",
    "                    lex_verb = ''\n",
    "                    for word in pred['span_wo_aux']:\n",
    "                        if word in pred['predicates']:\n",
    "                            lex_verb += str(word._.inflect('VB')).lower()+' '\n",
    "                        else:\n",
    "                            lex_verb += word.text+' '\n",
    "                        \n",
    "                    correction = ' '.join([before, aux, subj['phrase'], between, lex_verb, after])\n",
    "\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                a2 = predicate[-1].i\n",
    "                a1 = predicate[0].i\n",
    "                if len(predicate) == 3:\n",
    "                    a2 = predicate[1].i\n",
    "                l = predicate[-1].i\n",
    "\n",
    "                if not a1 < subj['start'] < a2 <= l:\n",
    "                    aux2 = ''\n",
    "                    aux1 = predicate[0].lower_\n",
    "                    lex_verb = pred['phrase_wo_aux']\n",
    "                    if len(predicate) == 3:\n",
    "                        aux2 = predicate[1].lower_\n",
    "\n",
    "                    correction = ' '.join([before, aux1, subj['phrase'], between, aux2, lex_verb, after])\n",
    "\n",
    "    \n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f49fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@clean_correction\n",
    "def standard_word_order(sent, pred_subj, conj_p):\n",
    "    '''SV'''\n",
    "    \n",
    "    correction = ''\n",
    "    \n",
    "    predicate = pred_subj[0]\n",
    "    pred = pred_chunk(sent, predicate, conj_p)\n",
    "    subject = pred_subj[1]\n",
    "    subj = subject_chunk(sent, subject)\n",
    "    \n",
    "    before, between, after = before_between_after(sent, pred, subj)\n",
    "        \n",
    "    if not subj['start'] < pred['start'] \\\n",
    "       and predicate[-1].dep_ != 'advcl' \\\n",
    "       and not (pred['span'][0].lemma_ in ['may', 'let'] and len(predicate) > 1):\n",
    "        # advcl for cases like Were she here, she would support me\n",
    "        # may and let for jussive cases\n",
    "        if len(predicate) == 1:\n",
    "            correction  = ' '.join([before, subj['phrase'], between, pred['phrase'], after])\n",
    "            \n",
    "        else:\n",
    "            aux2 = ''\n",
    "            aux1 = predicate[0]\n",
    "            if len(predicate) == 3:\n",
    "                aux2 = predicate[1].lower_\n",
    "                        \n",
    "            if aux1.lemma_ == 'do':\n",
    "                for i in pred['predicates']:\n",
    "                    s = i._.inflect(aux1.tag_)\n",
    "                    pred['phrase_wo_aux'] = re.sub(r'(?<=\\b)'+i.text+r'(?=\\b)', s, pred['phrase_wo_aux'], count=1)\n",
    "                aux1 = ''\n",
    "            else:\n",
    "                aux1 = aux1.lower_\n",
    "                \n",
    "            correction = ' '.join([before, subj['phrase'], aux1, between, aux2, pred['phrase_wo_aux'], after])\n",
    "    \n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c65546f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    correction = ''\n",
    "    ps, conj_p = find_pred_subj(doc)\n",
    "    ps = [ps_ for ps_ in ps if ps_[1]]\n",
    "\n",
    "    if ps:\n",
    "        q_mark = False\n",
    "        for token in doc:\n",
    "            if token.text == '?': q_mark = token.i\n",
    "        ps_before_q = []\n",
    "        for ps_ in ps:\n",
    "            if ps_[0][-1].i < q_mark: ps_before_q += [ps_]\n",
    "                \n",
    "        if q_mark:\n",
    "            # there is a question in the sentence\n",
    "            pred_subjs = [next(ps_ for ps_ in ps_before_q[::-1] \n",
    "                               if ps_[0][-1].dep_ not in ['advcl', 'relcl'] and ps_[0][-1].head.dep_ not in ['advcl', 'relcl'])]\n",
    "            pred_subjs += list(ps_ for ps_ in ps\n",
    "                               if ps_[0][-1] in pred_subjs[0][0][-1].conjuncts\n",
    "                                  and all(c not in [ch.lower_ for ch in ps_[0][-1].children] for c in ['but', 'so']))\n",
    "            new_corr = ''\n",
    "            for i in range(len(pred_subjs)):\n",
    "                if i <= len(pred_subjs)-1 and pred_subjs[i][1]:\n",
    "                    # checking i <= is necessary because the length of pred_subjs may change within the cycle\n",
    "                    new_corr = interrogative_WO(doc, pred_subjs[i], conj_p)\n",
    "                    if new_corr:\n",
    "                        correction = new_corr\n",
    "                        doc = nlp(correction)\n",
    "                        ps, conj_p = find_pred_subj(doc)\n",
    "                        ps = [ps_ for ps_ in ps if ps_[1]]\n",
    "                        pred_subjs = [next(ps_ for ps_ in ps_before_q[::-1] if ps_[0][-1].dep_ not in ['advcl', 'relcl'] \n",
    "                                           and ps_[0][-1].head.dep_ not in ['advcl', 'relcl'])]+\\\n",
    "                                      list(ps_ for ps_ in ps\n",
    "                                          if (ps_[0][-1] in pred_subjs[0][0][-1].conjuncts and\n",
    "                                          all(c not in [ch.lower_ for ch in ps_[0][-1].children] for c in ['but', 'so'])))\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            for i in range(len(ps)):\n",
    "                if i <= len(ps)-1:\n",
    "                    # checking i <= is necessary because the length of pred_subjs may change within the cycle\n",
    "                    p = ps[i][0][-1]\n",
    "                    for child in p.lefts:\n",
    "                        if (child.dep_ == 'neg' or \\\n",
    "                           child.lower_ in NON_NEGATIVE_INVERSION):\n",
    "                            new_corr = possible_inversion(doc, ps[i], conj_p, child)\n",
    "                            if new_corr:\n",
    "                                correction = new_corr\n",
    "                                doc = nlp(correction)\n",
    "                                ps, conj_p = find_pred_subj(doc)\n",
    "                                ps = [ps_ for ps_ in ps if ps_[1]]\n",
    "                                \n",
    "                if i <= len(ps)-1:                                \n",
    "                    if not ('ADV' in [t.pos_ for t in p.lefts] or any(i in [t.dep_ for t in p.lefts] for i in ['expl', 'prep'])):\n",
    "                        # clauses with adverbs first (e.g. here, there and so) can have both inversion and standard word order, \n",
    "                        # so we don't consider those\n",
    "                            new_corr = standard_word_order(doc, ps[i], conj_p)\n",
    "                            if new_corr:\n",
    "                                correction = new_corr\n",
    "                                doc = nlp(correction)\n",
    "                                ps, conj_p = find_pred_subj(doc)\n",
    "                                ps = [ps_ for ps_ in ps if ps_[1]]\n",
    "                                \n",
    "            subject_spans = []\n",
    "            for ps_ in ps:\n",
    "                if ps_[1]: \n",
    "                    span = subject_chunk(doc, ps_[1])['span']\n",
    "                    subject_spans += [word for word in span]\n",
    "            predicates = [ps_[0][-1].i for ps_ in ps]\n",
    "            for root, conjs in conj_p.items():\n",
    "                predicates += conjs\n",
    "\n",
    "            for w in doc:\n",
    "                \n",
    "                if (w.lower_ in ['nowhere', 'only'] \n",
    "                    and w not in subject_spans \n",
    "                    and w.head.i not in predicates) \\\n",
    "                   or (w.lower_ == 'no' \n",
    "                       and not w.is_sent_end\n",
    "                       and not w.is_sent_start \n",
    "                       and w.nbor().lower_ in ['way', 'circumstances', 'condition', 'conditions', 'point'] \n",
    "                       and w.nbor(-1).lower_ in ['under', 'in', 'on', 'at']) \\\n",
    "                   or (w.lower_ == 'no' \n",
    "                       and not w.is_sent_end \n",
    "                       and w.nbor().lower_ == 'sooner'):\n",
    "                    pred_to_inspect, w_c = None, w\n",
    "                    while not pred_to_inspect:\n",
    "                        p = w_c.head\n",
    "                        if p.i in predicates and p.dep_ not in ['advcl', 'relcl']:\n",
    "                            pred_to_inspect = p\n",
    "                        else:\n",
    "                            if w_c == p:\n",
    "                                pred_to_inspect = w # so that it doesn't loop indefinitely\n",
    "                            w_c = p\n",
    "                    if pred_to_inspect.i > w.i:\n",
    "                        try:\n",
    "                            ps_to_inspect = next(p for p in ps if p[0][-1] == pred_to_inspect)\n",
    "                            new_corr = possible_inversion(doc, ps_to_inspect, conj_p, w)\n",
    "                            if new_corr:\n",
    "                                correction = new_corr\n",
    "                                doc = nlp(correction)\n",
    "                                ps, conj_p = find_pred_subj(doc)\n",
    "                                ps = [ps_ for ps_ in ps if ps_[1]]\n",
    "                        except StopIteration:\n",
    "                            pass\n",
    "                        \n",
    "                        \n",
    "    return correction\n",
    "                \n",
    "\n",
    "NON_NEGATIVE_INVERSION = {'hardly', 'scarcely', 'barely', 'rarely', 'little', 'seldom'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe59397",
   "metadata": {},
   "source": [
    "Enter the text you want to correct below or copy and paste it into the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88274007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us discuss pluses and minuses about it and what should government do.\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "# text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8427055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bae84f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = ''\n",
    "for sent in text:\n",
    "    sent = re.sub(\"’|‘\", \"'\", sent)\n",
    "    corr = main(sent)\n",
    "    if corr:\n",
    "        final += ' '+corr\n",
    "    else:\n",
    "        final += ' '+sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1858bd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let us discuss pluses and minuses about it and what government should do.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = final.strip()\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4226604",
   "metadata": {},
   "source": [
    "Or if you want to see the difference highlighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7000648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Let us discuss pluses and minuses about it and what should government do.\n",
      "?                                                     -------\n",
      "\n",
      "+ Let us discuss pluses and minuses about it and what government should do.\n",
      "?                                                                +++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from difflib import Differ\n",
    "diff = Differ()\n",
    "\n",
    "for sent in text:\n",
    "    sent = re.sub(\"’|‘\", \"'\", sent)\n",
    "    corr = main(sent)\n",
    "    if corr:\n",
    "        print('\\n'.join(diff.compare([sent], [corr])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e9c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
